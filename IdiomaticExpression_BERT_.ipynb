{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cvillanue/DeepLearning-IdiomaticExpression/blob/main/IdiomaticExpression_BERT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7efrIsz6uPeQ"
      },
      "source": [
        "# **Deep Learning Based Idiomatic Expression Recognition using BERT**\n",
        "\n",
        "## Project developed by: Callyn Villanueva \n",
        "\n",
        "Article + peer-reviewed sources used to : Rani Horev, Rob Toews.\n",
        "[A New Approach for Idiom Identification Using Meanings and the Web](https://aclanthology.org/R15-1087) (Verma & Vuppuluri, RANLP 2015)\n",
        "\n",
        "About the EPIE Corpus Dataset: \n",
        "https://arxiv.org/abs/2006.09479 \n",
        "\n",
        "This dataset contains possible idiomatic expressions instances from 717 idioms divided into two folders:\n",
        "\n",
        "    Formal Idioms - Idioms which undergo lexical changes.\n",
        "\n",
        "    Static Idioms - Idioms which stay the same across instances.\n",
        "\n",
        "Each folder contains 3 sentence aligned files with '*' replaced with either 'Static_Idioms' or 'Formal_Idioms'\n",
        "*_Words.txt :- Original Sentences\n",
        "*_Candidates.txt :- Candidate Idiom whose instance is present in the corresponding sentence.\n",
        "*_Tags.txt :- Sequence labelling tags for each token of the sentence. Each entry delimited by space is treated as a separate token. The labelling follows BIO convention using three tags (B-IDIOM,I-IDIOM,O).\n",
        "\n",
        "    B-IDIOM:- beginning of possible idiomatic expression span\n",
        "    I-IDIOM:- continuation of possible idiomatic expression span\n",
        "    O:- Non-Idiom token\n",
        "\n",
        "For this project, I will be using BERT (Bidirectional Encoder Representations from Transformers) and will test Static Idioms. The model is designed to output binary classification, where each instance can be classified into one of two possible classes. In the case of idiom recognition, the model is trained to classify each instance as either an idiom or not an idiom.\n",
        "\n",
        "## Introduction: \n",
        "Language enables us to reason abstractly, to develop complex ideas about what the world is and could be, and to build on these ideas across generations and geographies. Almost nothing about modern civilization would be possible without language. One form of language we use is called **Idiomatic Expressions.** They are used to communicate or convey a feeling or emotion.  \n",
        "\n",
        "\n",
        "Building machines that can understand this form of language has been a complex problem, particulary with the usage and understanding of it. \n",
        "\n",
        "\n",
        "So, what are idioms? They’re a type of figurative language. You can’t rely on the words in an idiom to tell you what the phrase means. That’s because they have a meaning that is different from the literal meanings of the individual words themselves. Let’s look at an example. When someone says *it’s raining cats and dogs*, they don’t mean that there are actual animals falling from the sky. It’s an idiom! The phrase means that it’s raining very heavily.\n",
        "\n",
        "\n",
        "Additionally, some idioms are context dependent. Example:\n",
        "\n",
        "*The fisherman broke the ice with his tool.*\n",
        "are we to believe that this is a very suave fisherman?\n",
        "\n",
        "Another question arises, **is it is possible to teach an AI to use idiomatic phrases to keep up with the culture of humans?**\n",
        "\n",
        "Observe that humans do not come linguistically \"pre-loaded\" with idioms. So we can safely assume that idiom usage is a learning task and that the only way for them to keep up is for them to keep learning. So if we solve the idiom learning task we just need to keep our agent online or periodically retrain it on nascent corpora. \n",
        "\n",
        "\n",
        "\n",
        "**About BERT:**\n",
        "\n",
        "BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary.\n",
        "\n",
        "\n",
        "**Masked LM (MLM)**\n",
        "\n",
        "Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n",
        "\n",
        "    Adding a classification layer on top of the encoder output.\n",
        "    Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
        "    Calculating the probability of each word in the vocabulary with softmax.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d95bSejz9LL"
      },
      "source": [
        "Generalized Steps to train Corpus Data using BERT:\n",
        "\n",
        "1. You will need to install the transformers library in Python, which provides a high-level interface for working with pre-trained transformer models such as BERT. You can install the library using pip by running the command pip install transformers.\n",
        "\n",
        "2. Load and Tokenize the Corpus Data: You need to load and tokenize the corpus data using the BertTokenizer class from the transformers library. This class tokenizes the text and maps the tokens to their corresponding IDs for use with BERT.\n",
        "\n",
        "3. Preprocess the Corpus Data: \n",
        " Load the data from the three files (*_Words.txt, *_Candidates.txt, and *_Tags.txt) and split the sentences into individual words/tokens.\n",
        "For each sentence, create a list of candidate idioms and their corresponding tags.\n",
        "Convert the list of candidate idioms and tags for each sentence into a feature vector that can be fed into the BERT model. This could involve using the BERT tokenizer to convert the tokens into BERT input IDs, segment IDs, and attention masks, and then creating a separate label vector for each candidate idiom indicating whether it is an idiom or not.\n",
        "\n",
        " \n",
        "4. Load the BERT Model: You can load a pre-trained BERT model from the transformers library using the BertForSequenceClassification class. This class provides a BERT model that has been pre-trained on a large corpus of text and can be fine-tuned for specific NLP tasks.\n",
        "\n",
        "5. Fine-tune the BERT Model: You can fine-tune the BERT model on your corpus data using a technique called transfer learning. This involves training the model on your corpus data for a specific NLP task such as sentiment analysis, text classification, or question-answering.\n",
        "\n",
        "6. Evaluate the BERT Model: After fine-tuning the model, you can evaluate its performance on a test dataset to measure its accuracy and other metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KXy5B_K0W-e",
        "outputId": "570183e2-00b8-4c65-e883-aee8334a6c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert in /usr/local/lib/python3.9/dist-packages (2.2.0)\n",
            "Requirement already satisfied: erlastic in /usr/local/lib/python3.9/dist-packages (from bert) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oso5O3WG1cSg",
        "outputId": "ce296570-fc7a-4287-a84c-3ca32bb58338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0zN-ei451xcP"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFRs7HIP2AeN",
        "outputId": "61063134-55ae-4f18-d0ed-da14eb4f18b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Loading the tokenizer and pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALtT3nDG_if7",
        "outputId": "179dc5fa-fdbc-4747-8ab4-14ee55ac7c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Formal_Idioms_Corpus.zip\n",
            "replace __MACOSX/._Formal_Idioms_Corpus? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: __MACOSX/._Formal_Idioms_Corpus  \n",
            "replace Formal_Idioms_Corpus/Formal_Idioms_Candidates.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace __MACOSX/Formal_Idioms_Corpus/._Formal_Idioms_Candidates.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace Formal_Idioms_Corpus/Formal_Idioms_Tags.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace __MACOSX/Formal_Idioms_Corpus/._Formal_Idioms_Tags.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace Formal_Idioms_Corpus/Formal_Idioms_Words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace __MACOSX/Formal_Idioms_Corpus/._Formal_Idioms_Words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace Formal_Idioms_Corpus/Formal_Idioms_Labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace __MACOSX/Formal_Idioms_Corpus/._Formal_Idioms_Labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n"
          ]
        }
      ],
      "source": [
        "!unzip Formal_Idioms_Corpus.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xar1UyC72OsW",
        "outputId": "6b513af9-09a2-41ee-e2b7-71310ade0ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "corpus_path = \"Formal_Idioms_Corpus/\"\n",
        "\n",
        "# create a list of file paths for all *_Words.txt files in the corpus\n",
        "corpus_files = [os.path.join(corpus_path, f) for f in os.listdir(corpus_path) if f.endswith(\"_Words.txt\")]\n",
        "\n",
        "# Create list of sentences and corresponding candidate idioms/tags\n",
        "sentences = []\n",
        "candidate_idioms = []\n",
        "tags = []\n",
        "\n",
        "# Iterate through each file and load data\n",
        "for words_path in corpus_files:\n",
        "    candidates_path = words_path.replace(\"_Words.txt\", \"_Candidates.txt\")\n",
        "    tags_path = words_path.replace(\"_Words.txt\", \"_Tags.txt\")\n",
        "    \n",
        "    with open(words_path, 'r') as words_file, \\\n",
        "         open(candidates_path, 'r') as candidates_file, \\\n",
        "         open(tags_path, 'r') as tags_file:\n",
        "        \n",
        "        words_lines = words_file.readlines()\n",
        "        candidates_lines = candidates_file.readlines()\n",
        "        tags_lines = tags_file.readlines()\n",
        "        \n",
        "        for words_line, candidates_line, tags_line in zip(words_lines, candidates_lines, tags_lines):\n",
        "            words = words_line.strip().split()\n",
        "            candidates = candidates_line.strip().split('\\t')\n",
        "            sentence_tags = tags_line.strip().split()\n",
        "\n",
        "            sentence_candidates = []\n",
        "            candidate_tags = []\n",
        "\n",
        "            # Iterate through each word in the sentence and create candidate idioms and tags\n",
        "            for i, tag in enumerate(sentence_tags):\n",
        "                if tag == 'B-IDIOM':\n",
        "                    # Start of a candidate idiom\n",
        "                    candidate = words[i]\n",
        "                    tag = 1  # 1 indicates idiom\n",
        "                    j = i + 1\n",
        "                    while j < len(sentence_tags) and sentence_tags[j] == 'I-IDIOM':\n",
        "                        # Add additional words to candidate idiom\n",
        "                        candidate += ' ' + words[j]\n",
        "                        sentence_tags[j] = 'O'  # Mark words as not part of candidate idiom\n",
        "                        j += 1\n",
        "                    sentence_candidates.append(candidate)\n",
        "                    candidate_tags.append(tag)\n",
        "                elif tag == 'O':\n",
        "                    # Not part of a candidate idiom\n",
        "                    sentence_candidates.append(words[i])\n",
        "                    candidate_tags.append(0)  # 0 indicates not idiom\n",
        "\n",
        "            sentences.append(words)\n",
        "            candidate_idioms.append(sentence_candidates)\n",
        "            tags.append(candidate_tags)\n",
        "\n",
        "print(type(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9YuVy7jEsqi",
        "outputId": "17b7179d-410f-4701-f0cd-4ddcca7868c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Convert candidate idioms and tags for each sentence into feature vectors\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "labels = []\n",
        "\n",
        "for i, sentence_candidates in enumerate(candidate_idioms):\n",
        "    for j, candidate in enumerate(sentence_candidates):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            candidate,\n",
        "                            add_special_tokens = True,\n",
        "                            max_length = 64,\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,\n",
        "                            return_token_type_ids = True,\n",
        "                            return_tensors = 'pt',\n",
        "                       )\n",
        "\n",
        "        # Convert tensor elements to int and append to respective lists\n",
        "        input_ids.append(encoded_dict['input_ids'].squeeze().tolist())\n",
        "        attention_masks.append(encoded_dict['attention_mask'].squeeze().tolist())\n",
        "        token_type_ids.append(encoded_dict['token_type_ids'].squeeze().tolist())\n",
        "        labels.append(int(tags[i][j]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9UZ-NX-A5bm",
        "outputId": "2f157731-8571-46af-b726-36514c2a6ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "with open(\"Formal_Idioms_Corpus/Formal_Idioms_Labels.txt\", \"r\") as f:\n",
        "    labels_list = [int(label.strip()) for label in f]\n",
        "\n",
        "print(labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl1T-4_VDvrD",
        "outputId": "88797ef2-566d-40d7-f84a-1d4968e7772b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List contains non-integer elements\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "if all(isinstance(elem, int) for elem in token_type_ids):\n",
        "    print(\"All elements are integers\")\n",
        "else:\n",
        "    print(\"List contains non-integer elements\") \n",
        "\n",
        "print(type(token_type_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zoKk4wvgObTY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# Convert feature vectors and label vectors into PyTorch tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "token_type_ids = torch.tensor(token_type_ids)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Create TensorDataset\n",
        "dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
        "\n",
        "# Define the fraction of data to be used for testing and validation\n",
        "test_fraction = 0.1 #remaining 90% will be used for training and validation\n",
        "validation_fraction = 0.2\n",
        "\n",
        "# Calculate the number of samples in the test and validation sets\n",
        "test_size = int(len(dataset) * test_fraction)\n",
        "validation_size = int(len(dataset) * validation_fraction)\n",
        "\n",
        "# Calculate the number of samples in the training set\n",
        "train_size = len(dataset) - test_size - validation_size\n",
        "\n",
        "# Use random_split to create the training, validation, and test datasets\n",
        "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "# Create DataLoaders for the training, validation, and test datasets\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4zVe4Pc0KGT",
        "outputId": "5b9e5692-3c3d-494c-93e1-8b9d849141ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([81289, 64])\n",
            "torch.Size([81289, 64])\n",
            "torch.Size([81289])\n"
          ]
        }
      ],
      "source": [
        "print(input_ids.shape)\n",
        "print(attention_masks.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhM_hAU6YLIn",
        "outputId": "33c6dcfc-107b-4565-ca15-bcdb94f0c5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "Successfully installed pandas-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "id": "fzPGsZoZYtyX",
        "outputId": "023ab437-e877-4126-b54c-5db3536e41ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "5G238gJIZxhZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "OxoaouW8zoSt",
        "outputId": "67b88539-e22a-4ab1-c935-bedce38b724c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Train the model\n",
        "epochs = 3\n",
        "train_losses = []\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Load batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack inputs and labels from batch\n",
        "        input_ids, attention_mask, token_type_ids, labels = batch\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = outputs.loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em9hnvR_N_XY",
        "outputId": "e7d360b8-3ad3-4cac-d780-258dece18e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train loss: 0.000 Train accuracy: 0.000 Validation accuracy: 1.000\n",
            "Test accuracy: 1.000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()\n",
        "eval_accuracy = 0\n",
        "eval_steps = 0\n",
        "for batch in validation_dataloader:\n",
        "    # Load batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack inputs and labels from batch\n",
        "    input_ids, attention_mask, token_type_ids, labels = batch\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        # Compute predictions\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "        eval_accuracy += accuracy\n",
        "        eval_steps += 1\n",
        "\n",
        "# Calculate average accuracy on validation set\n",
        "eval_accuracy /= eval_steps\n",
        "train_accuracy = 0\n",
        "# Print training and validation accuracies for each epoch\n",
        "print(f\"Epoch {epoch+1}: Train loss: {loss.item():.3f} Train accuracy: {train_accuracy:.3f} Validation accuracy: {eval_accuracy:.3f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = 0\n",
        "test_steps = 0\n",
        "for batch in test_dataloader:\n",
        "    # Load batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack inputs and labels from batch\n",
        "    input_ids, attention_mask, token_type_ids, labels = batch\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        # Compute predictions\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "        test_accuracy += accuracy\n",
        "        test_steps += 1\n",
        "\n",
        "# Calculate average accuracy on test set\n",
        "test_accuracy /= test_steps\n",
        "\n",
        "# Print test accuracy\n",
        "print(f\"Test accuracy: {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def process_input(input_text, max_seq_length=100):\n",
        "    if isinstance(input_text, list):\n",
        "        input_text = ' '.join(input_text)\n",
        "    words = input_text.split()\n",
        "    input_ids = np.zeros((1, max_seq_length), dtype=np.int32)\n",
        "    for i, word in enumerate(words[:max_seq_length]):\n",
        "        if word in tokenizer.vocab:\n",
        "            input_ids[0][i] = tokenizer.vocab[word]\n",
        "\n",
        "    return torch.tensor(input_ids).to(device)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "input_string = input('Enter a phrase: ')\n",
        "input_tensor = process_input(input_string)\n",
        "output_tensor = model(input_tensor)[0]\n",
        "\n",
        "output = output_tensor.tolist()[0]\n",
        "if output[0] > 0.5:\n",
        "    print('Positive (Idiom)')\n",
        "else:\n",
        "    print('Negative (non-idiom)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyxCk0emqkNr",
        "outputId": "d05fd7a0-71c6-4bf6-97a0-fb3cb762646e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a phrase: It's raining outside\n",
            "Positive (Idiom)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME7eP2mFc8iRuq3JdzhqWV",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}